{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Collaborative Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PACMAN: A Comparative Analysis of Reinforcement Learning Approaches (DQN, SARSA, and Monte Carlo)\n",
    "\n",
    "## Members:\n",
    "- Charlene Hsu\n",
    "- Ananya Krishnan\n",
    "- Ava Jeong \n",
    "- John Wesley Pabalate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "We tested different reinforcement learning models to train an agent to play Ms. Pac-Man. The data consists of game states from the Gymnasium Ms. Pac-Man environment, where each observation includes Pac-Man’s position, ghost locations, and pellet positions. We compared 4 models – a baseline random agent, SARSA, Monte Carlo Control and DQN. As expected, the baseline showed no learning or improvement and acted as a benchmark for other models. SARSA, an on-policy model, performed the worst, showing almost negative improvement. The Monte Carlo Control model performed slightly better showing very weak improvement. In contrast, DQN performed best amongst all models, but exhibited some fluctuation due to exploration-exploitation tradeoff. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Reinforcement learning allows agents to learn the best techniques through trial and error, which has made it a powerful tool for game AI and decision making. In contrast to supervised learning, where models are based on labeled datasets, reinforcement learning enables an agent to engage with its surroundings, get feedback in the form of rewards or penalties, and gradually improve its behavior <a name=\"lorenz\"></a>[<sup>[3]</sup>](#sutton). The dynamic surroundings, strategic planning requirements, and real-time decision-making limits of games like Pac-Man make them an intriguing RL challenge. \n",
    "\n",
    "Pac-Man is a grid-based maze game that introduces multiple competing objectives: maximize score, avoid ghosts, and utilize power pellets strategically. The complexity of the game lies in its non-deterministic ghost movements, limited visibility, and real-time constraints. Classic AI approaches to Pac-Man lack adaptability when faced with unexpected situations <a name=\"lorenz\"></a>[<sup>[1]</sup>](#koenig). RL offers a better alternative by enabling Pac-Man to learn from experience, adjusting movement strategies dynamically instead of relying on predefined rules. \n",
    "\n",
    "A key factor in RL’s success in game environments is its state-action-reward structure, where the agent processes observations (Pac-Man’s position, pellet locations, ghost behavior) and selects actions (Up, Down, Left, Right) to maximize cumulative rewards. Deep reinforcement learning (DRL) has shown promising results in complex games, enabling AI to generalize across different gameplay scenarios <a name=\"lorenz\"></a>[<sup>[2]</sup>](#mnih).\n",
    "\n",
    "One of the critical challenges in RL for Pac-Man is handling enemy AI behavior, which can range from simple random movements to sophisticated strategies where ghosts actively chase Pac-Man. Ghosts operate in different models:\n",
    "Random Movement: Ghosts move without a fixed pattern.\n",
    "\n",
    "Rule-Based Chasing: Ghosts follow Pac-Man based on predefined logic.\n",
    "Mixed Strategies: Ghosts switch between random and targeted movement, making the game environment less predictable.\n",
    "This dynamic opponent behavior requires the RL agent to adapt its decision-making in real-time, enhancing its strategic learning process. Researchers have shown that incorporating multi-agent RL (MARL) in Pac-Man—where both Pac-Man and ghosts operate as independent learning agents—can create emergent behaviors that closely resemble human-like strategies (Hu et al., 1998). This leads to more challenging and engaging gameplay, further pushing the boundaries of AI-driven game intelligence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We want to use the Gymnasium environment of the Atari games’ Pacman to build an iteration of the Pacman game that optimizes navigational strategies in a dynamic maze-like environment using reinforcement learning. Our implementation uses the default Gymnasium Ms. Pac-Man environment, where power pellets temporarily allow Pac-Man to eat ghosts. We do not manually modify ghost behavior but rely on the existing environment dynamics. Our goal is to maximize the rewards by collecting all pellets while navigating through the maze and learning to avoid obstacles, which are ghosts that penalize the agent. \n",
    "\n",
    "We will evaluate different Reinforcement Learning approaches against a baseline random agent. The Pacman environment from Gymnasium is used to define the environment that the agent is trained in, defining the state, action, reward, and transitions for this environment. And the RL algorithms train the agent to optimize the total rewards as it navigates through the maze using varying optimal policies.\n",
    "\n",
    "The problem is: \n",
    "Quantifiable, since the rewards collected through navigating throughout the maze can be tracked and the epsilon values can be collected to examine the epsilon decay strategy in play, where the algorithm can be concluded to be learning or not due to decreasing or increasing epsilon values. \n",
    "Measurable, since it evaluates the models’ effectiveness throughout the game through the agent’s performance of collecting pellets, avoiding ghosts, and understanding the dynamics of the game as it learns, which can be measured through looking at the total rewards collected in each episode.\n",
    "Replicable, where the various models can be trained on various optimization policies respective to different reinforcement learning models, where the agent has to learn how to navigate through these varying configurations for learning.\n",
    "\n",
    "The challenge of this project comes from the aspect of balancing the exploration and exploitation through an epsilon-greedy strategy of the agent to optimize the rewards, as well as finding optimal agents that are able to be seen continuously learning through having the total rewards collected become increasingly higher with more episodes that are run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We will be implementing 3 different reinforcement learning models that will observe the environment state (Pac-Man’s position, ghost locations, pellet positions), select an action (move up, down, left, right), receive feedback (rewards for collecting pellets, penalties for ghost collisions) and update learning models to improve future decisions.\n",
    "The models we will evaluate are:\n",
    "1. SARSA (On-Policy Temporal Difference)\n",
    "- Updates Q-values incrementally during gameplay, refining policy using a stepwise learning process.\n",
    "- Uses Q-learning updates in real time: \n",
    "\n",
    "    ![](image/formula1.png)\n",
    "- Exploration-exploitation tradeoff via epsilon decay\n",
    "\n",
    "\n",
    "2.  Monte Carlo Control (MC)\n",
    "- Uses episodic learning, where Q-values are updated only at the end of each episode based on the total reward collected. \n",
    "\n",
    "\n",
    "    ![](image/formula2.png)\n",
    "\n",
    "- Q-value estimation via experience replay\n",
    "- Epsilon-greedy policy for action selection\n",
    "\n",
    "3. Deep Q-Network (DQN)\n",
    "- Uses neural networks to approximate Q-values, allowing learning in high-dimensional state spaces.\n",
    "- Neural network architecture:\n",
    "    - Input: Game state (RAM pixels or raw observations)\n",
    "    - Hidden layers: Fully connected dense layers\n",
    "    - Output: Q-values for each action\n",
    "- Replay buffer for efficient experience reuse\n",
    "- Target network to stabilize learning\n",
    "\n",
    "4. Benchmark Model\n",
    "- Our baseline model was one that was implemented in the way that Pacman moves based on random movements. Essentially, the agent is designed to choose actions randomly without learning where it is going around the maze and the dynamics of the environment, a random agent that functions under a random policy. This provided a benchmark to compare other models with. All other models should outperform this.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The performance of the model will be evaluated on the following:\n",
    "- Cumulative Reward: Represents the total rewards collected over an episode.\n",
    "    - Calculating the total rewards for an episode allows the agent’s performance to be directly measured as higher cumulative rewards would show that the agent is navigating the environment, collecting pellets, and avoiding ghosts more effectively compared to an agent with lower cumulative rewards. Through tracking the total rewards over all episodes run, the agent can be determined if it is truly learning and improving its navigation through the environment, where an increase in the total rewards over episodes would show that the model is being optimized to the environment.\n",
    "    \n",
    "        ![](image/cumulative_rew.png)\n",
    "\n",
    "- Convergence of Q-values: Represents the reduction in policy over episodes.\n",
    "The stabilization of the Q-values as the agent is optimized to an optimal policy can be used to observe how the agent is learning to become more optimal in navigating the environment, where the Q-table is no longer being updated significantly. If the Q-values converge and the Q-table is not being significantly updated, then the agent can be concluded to have reached an optimal policy in navigating the environment and vice versa.\n",
    "- Epsilon Decay: Represents the change in epsilon values over episodes.\n",
    "The reduction in the epsilon values, or the exploration rate, over episodes can be used to depict how the agent is learning as it shifts from exploration to exploitation, to further analyze how the models balance exploration and exploitation. As epsilon values decrease, the agent can be seen shifting from exploration of the environment randomly to exploitation as it uses what it has learned to optimize the rewards.\n",
    "- Comparison to benchmark: Compare rule-based Pac-Man agent with random policies. \n",
    "As reinforcement learning models that implement policy-based agents are compared to agents that run on random policies of exploration, the policy-based agents can be shown to be maximizing its rewards and show how learned policies are superior. The baseline is used as a control to verify that the reinforcement learning agent is not randomly exploring the environment, but using what it has learned to navigate and continuously learn.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "1. **Our baseline model** was one that was implemented in the way that Pacman moves based on random movements. Essentially, the agent is designed to choose actions randomly without learning where it is going around the maze and the dynamics of the environment, a random agent that functions under a random policy. This provided a benchmark to compare other models with.\n",
    "\n",
    "    ![](image/baseline.png)\n",
    "Caption: As seen in the training progress graph of the Random Agent in Ms. Pac-man, the total reward throughout 100 episodes is scattered in values, where most total reward values range around 200 to 400. This remains pretty consistent throughout all 200 episodes with no visible trend. The epsilon value is constant, where there are no fluctuations in the epsilon value within the agent, having a set value of 0.5. The agent never transitioned from exploration to exploitation. Pac-man moved erratically and would collide with ghosts often. Sometimes it achieved high rewards \n",
    "\n",
    "\n",
    "\n",
    "2. Our **SARSA model** was implemented to allow Pacman to learn from its previous experiences using a policy that updates Q-values through on-policy learning. The agent (Pacman) balances both exploration and exploitation by adjusting its action on learned Q-values while also exploring different paths to avoid less optimal policies. The training process involves updating the Q-values with the SARSA update rule as well as gradually reducing the epsilon value to shift from exploration to exploitation as learning progresses. The agent (Pacman) was trained over 500 episodes, allowing it to refine its decision-making process and develop an enhanced navigation strategy over time. \n",
    "\n",
    "    ![](image/sarsa.png)\n",
    "Caption: As seen in the training progress graph of the SARSA agent, the total reward per episode fluctuates significantly. However, the moving average does not show a strong upward trend, suggesting that the agent’ learning progress is very inconsistent. While there are periods where the rewards appear to increase, the overall pattern remains highly variable, which shows that the agent (Pacman) is not consistently improving its strategy over 500 episodes. The epsilon value from the left graph shows an exponential decrease, meaning the agent gradually shifts from exploration to exploitation, but this does not directly translate to steady performance gains. The result suggests that SARSA does not show effective learning, and the reward scores seem to even be decreasing a little. Unlike the random agent, SARSA still demonstrates some structure in its behavior, avoiding obstacles and making strategic choices, but the slightly negative trend implies that further tuning or additional training episodes may be necessary for stronger policy refinement. \n",
    "\n",
    "3. Monte Carlo Control:\n",
    "We selected **Monte Carlo Control** due to its ability to estimate Q-values directly from episodic returns, making it well-suited for environments where full trajectories are available. It relies on first-visit updates to approximate Q-values and the agent was trained over 200 episodes. The total reward per episode showed high variance throughout training. \n",
    "    ![](image/monte_carlo.png)\n",
    "\n",
    "The agent performed inconsistently and experienced random spikes in ability due to chance rather than learning. The reward curve did not show a strong upward trend. However, looking at the moving average, there seemed to be a subtle upward trend that was not otherwise visible, suggesting some improvement over time. The agent followed an ε-greedy policy, with ε decaying from 1.0 to 0.9 in this time, to shift from exploration to exploitation. \n",
    "\n",
    "4. Deep Q-Network\n",
    "With our **Deep Q-Network (DQN) model**, we approximate the Q-values through a neural network trained with temporal-difference updates on Pacman, relying on a replay buffer and target network to stabilize training. Pacman (agent) was trained with a repetition of 500 episodes, during which it observed transitions and updated its Q-function accordingly. \n",
    "    ![](image/monte_carlo.png)\n",
    "\n",
    "The agent's total reward per episode fluctuated significantly, but also displayed some random spikes, suggesting that it would randomly learn more as it continues to repetitively optimize its surroundings, and try to survive. As expected, the ε decreased per episode creating a logarithmic decay curve that allowed the agent to transition gradually from fully optimizing its environment. Overall, the trajectory of the reward per episode does show a sequential performance of learning; it’s not the best, but the learning gets better and better throughout each episode. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "    \n",
    "<p align=\"center\">\n",
    "  <img src=\"image/pacman.png\" alt=\"Pacman Screenshot\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "#### **Baseline Model Performance:**\n",
    "\n",
    "Based on the Training Progress graph for the Random Agent, this baseline model can be said to perform rather poorly in the environment, where there is no clear learning and improvement within the environment. Through this graph and observations of the Random Agent, a result that can be interpreted is that this agent is not learning throughout these episodes, which is due to the characteristics of how it randomly chooses its actions.\n",
    "As for the Epsilon Progress graph, a constant line graph is depicted because there is no no exploration-exploitation strategy being implemented because the random agent always runs on exploration, therefore, the epsilon value is set to a constant 0.5.\n",
    "The same goes for the reasons why there are no Q-values that are being plotted because the random agent does not implement and update Q-values because there is no learning involved except for random exploration of the environment.\n",
    "Because of these observations, the Pacman Random Agent is seen to be sporadically exploring the maze within the Ms. Pacman environment through random actions with no clear direction that the agent is moving in. Because of these random movements and actions, the Pacman Random Agent is easily captured by the ghosts and loses its life pretty quickly. Because of this, the total rewards collected is generally not significantly great, other than random chances that the agent is able to coincidentally move through the environment while avoiding the ghosts. This occurred on pretty rare occasions, as seen on the Training Progress graph, there are very random spikes that show randomly high total rewards that the agent collected in that episode.\n",
    "Because of these evaluations, this is determined as the baseline model and can be used to compare and be evaluated against further advanced reinforcement learning agents like DQN, sarsa, and Monte Carlo. This baseline model helps to show the difference between unoptimized models and optimized models, further highlighting the learning in the reinforcement learning models.\n",
    "\n",
    "#### **SARSA:**\n",
    "\n",
    "The SARSA model did not demonstrate a clear trend of improvement over 500 episodes, as seen in the training progress graph.While the random agent showed no signs of learning and Monte Carlo exhibited high variance, SARSA’s results were inconsistent, suggesting that the agent struggled to converge toward an optimal policy. The-on policy nature of the model allowed for some level of structured learning, though the fluctuations in total reward indicate that the model’s decision-making was not improving. \n",
    "\n",
    "One key observation from the results is that the SARSA’s total rewards varied widely without a strong upward trend. While some episodes show significant higher rewards, there are also periods of low performance, preventing a clear indication of stabilized learning. The exponential decrease in the epsilon decay suggests that the agent gradually transitioned from exploration to exploitation, but this does not directly translate to consistent performance gains. Instead, SARSA’s updates led to oscillations in reward, indicating that the agent sometimes made optimal decisions but struggled to refine its policy. \n",
    "\n",
    "The SARSA model is more conservative as it updates Q-values based on the actual action taken rather than the maximum future reward, which is what the Q-learning model does. Due to this, the model resulted in a lower learning curve and inconsistent improvements, as the agent may have settled into locally optimal but not globally optimal strategies. While SARSA was better than the random exploration, the lack of a strong trend suggests that additional hyperparameter tuning or training episodes may be necessary to draw a more conclusive progress.\n",
    "\n",
    "When comparing the performance of the SARSA model and Deep Q-Networks (DQN), the model’s performance was less effective in boosting rewards over time. While SARSA was able to develop some level of structured decision-making, its learning progress was slower and overall highly variable. Overall, we cannot say if SARSA outperformed the random agent. Its performance gains were highly unstable and did not demonstrate a strong learning curve. \n",
    "\n",
    "#### **Monte Carlo Control:**\n",
    "Monte Carlo Control exhibited high variance, meaning that while it occasionally outperformed the random agent, it was not consistently better. Compared to DQN and SARSA, Monte Carlo was the least effective method, requiring longer training to reach comparable performance. The poor learning was likely due to a lack of generalization, which prevented the model from forming a coherent long-term strategy. This limitation was a result of Monte Carlo's episodic updates, where each episode was treated independently, restricting experience reuse.\n",
    "Monte Carlo waits until the end of each episode to update, making learning slower, and leading to gradual and unstable convergence. While our original plan was to train the agent over 500 episodes, the process was computationally expensive, with 200 episodes requiring over 12 hours of runtime.\n",
    "\n",
    "#### **Deep Q-Networks:**\n",
    "According to the training plot of our DQN agent, the rewards per episode go up and down, but overall they get better until the maximum threshold is reached. In the beginning, the agent tries out many different actions, which leads to a wide range of scores from one episode to the next. As it learns which moves work best and the exploration rate drops, it starts making smarter decisions and the scores gradually improve. Even though the rewards still seem a bit chaotic, with some episodes scoring high and others scoring low. However the general trend shows that the DQN is picking up useful strategies. When we compare it to SARSA and Monte Carlo methods, the DQN seems to do a much better job of boosting the average reward over time. Monte Carlo’s performance was weak within 200 episodes, but longer training may improve its results. Overall, the DQN appears to be more capable of learning the key patterns needed to succeed in making Pacman smarter, and while there’s still room for further tuning and improvement, these results are good enough.\n",
    "\n",
    "#### **Limitations**\n",
    "Pacman displays rapid oscillations (erratic movement) occurring due to rapid direction changes. This happens when the agent is frequently recalculating its moves and looking for the most optimal route, leading to indecisiveness and oscillations as it's trying to decide between the conflicting options. This could be caused by the following:\n",
    "Overfitting →  The agent is prioritizing short-term rewards, causing it to frequently switch between similar actions.\n",
    "Lack of action commitment → There is no mechanism being put in place to ensure that the agent sticks with the decision. \n",
    "No Penalization for Rapid Switching → The algorithm does not discourage erratic movement patterns, only focusing on reward collection and obstacle avoidance. \n",
    "The agent doesn’t have long-term planning abilities or full knowledge of the maze since the algorithm relies on rewards and only considers relative positions of the ghosts and the pellets within a limited range. This prevents planning multistep paths.\n",
    "No transfer of learning\n",
    "This means that the agent is trained to navigate a specific maze layout, it learns a policy that is optimized for that exact environment. As a result, this learned knowledge does not transfer when the layout of the maze changes. For each new maze, the agent must go through the entire training process from scratch. \n",
    "Another limitation is many of the Reinforcement Learning algorithms take hours to be able to run enough episodes to show actual learning from the agent, whereas the costs of running these models greatly limit the ability to fully run enough episodes to see the most optimized learning from the agent.\n",
    "\n",
    "#### **Future Work**\n",
    "There are many ways to improve our Pacman agent even further. One idea is to help the agent better remember its past moves. Right now, it learns from each episode, but if we can make it store and review its past routes and experiences, it might avoid repeating mistakes and find the best paths through the maze more quickly. This could mean adding a better memory system or using an enhanced experience replay mechanism that pays extra attention to moves that lead to higher rewards. Another idea is to help the agent plan ahead. Instead of just reacting to the current situation, it could look a few moves into the future. With some forward planning, it might avoid traps or better escape from ghosts, which could lead to higher overall rewards.  \n",
    "\n",
    "We also want to explore other learning techniques beyond basic Q-learning. For example, deep Q-networks (DQN) have already shown promising results by allowing the agent to process more detailed information about the maze. Future work could include trying methods like Double Q-Learning or Proximal Policy Optimization (PPO), which might help the agent learn in a steadier and more reliable way. These advanced methods could make the learning process faster and more stable, even in the complex and unpredictable environment of Ms. Pacman.  \n",
    "\n",
    "Additionally, we plan to run more experiments and compare a wider range of learning models. By testing different approaches side by side, we can figure out which ones perform best in various situations. This kind of detailed comparison can help us understand what works well and where each method might need more tuning. It might also lead us to combine the strengths of different techniques into a more powerful hybrid model.  \n",
    "\n",
    "Lastly, we need to address practical challenges like reducing the training time. Reinforcement learning models often require many hours of training, so finding ways to optimize the process, such as updating the agent more efficiently or reducing unnecessary computations. All these improvements aim to create a Pacman agent that is not only better at surviving longer but also plays in a smarter and more strategic way. These steps could also pave the way for applying our methods to other games and real-world scenarios, making our work useful beyond Pacman.\n",
    "\n",
    "#### **Ethics and Privacy**\n",
    "Since this project involves us developing the game of Pac-Man and does not require external data sources, privacy concerns are not as important. However, we must take ethical considerations into account in reinforcement learning applications. \n",
    "\n",
    "One potential concern would be the use of reinforcement learning agents displaying biased behavior based on the training environments. To prevent this from occurring, we will closely monitor the learning process to ensure that the agent’s decision-making aligns with the intended gameplay strategies. \n",
    "\n",
    "By carefully designing the reward structures and training constraints, we can ensure that the agent learns an optimal strategy for gameplay while maintaining fairness, and adhering to the ethical decision-making principles.\n",
    "\n",
    "#### **Conclusion**\n",
    "Our results show that DQN performed the best of all models, showing consistent improvement and stable learning. SARSA performed the worst of all models, with a slightly negative trend in learning. Monte Carlo did slightly better, performing similarly to our baseline model. This is consistent with literature where DQN has been widely recognized for its ability to approximate Q-values efficiently in high-dimensional state spaces. However, our results also highlight the inefficiency of Monte Carlo methods in environments with long episodes and high variance. \n",
    "Our results highlight the effectiveness of DQNs in learning optimal policies in dynamic environments while reinforcing the limitations of Monte Carlo methods in short training windows. These findings align with broader reinforcement learning research, where deep learning-based RL techniques have outperformed traditional tabular approaches. Future work should focus on extending training durations and refining hyperparameters to improve learning efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"shillernote\"></a>[<sup>[1]</sup>](#Koenig) Koenig, S., and M. Likhachev.\n",
    "\"D* Lite.\" AAAI Conference on Artificial Intelligence, 2002. https://cdn.aaai.org/AAAI/2002/AAAI02-072.pdf\n",
    "\n",
    "<a name=\"shillernote\"></a>[<sup>[2]</sup>](#Mnih) Mnih, V., et al.\n",
    "\"Human-Level Control through Deep Reinforcement Learning.\" Nature, vol. 518, no. 7540, 2015, pp. 529–533.https://www.nature.com/articles/nature14236\n",
    "\n",
    "<a name=\"shillernote\"></a>[<sup>[3]</sup>](#sutton) Sutton, R. S., and A. G. Barto.\n",
    "Reinforcement Learning: An Introduction. MIT Press, 2018. https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
