{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "You have the choice of doing either (1) an AI solve a problem style project or (2) run a Special Topics class on a topic of your choice.  If you want to do (2) you should fill out the _other_ proposal for that. This is the proposal description for (1).\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like 8-Queens or a small Traveling Salesman Problem or similar\n",
    "- If its the kind of problem (e.g., RL) that interacts with a simulator or live task, then the problem will have a reasonably complex action space. For instance, a wupus world kind of thing with a 9x9 grid is definitely too small.  A simulated mountain car with a less complex 2-d road and simplified dynamics seems like a fairly low achievement level.  A more complex 3-d mountain car simulation with large extent and realistic dynamics, sure sounds great!\n",
    "- If its the kind of problem that uses a dataset, then the dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training an unsupervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "- The project must include some elements we talked about in the course\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your AI system. Generally RL tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible. \n",
    "- You will evaluate the performance of your AI system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Charlene Hsu\n",
    "- Ava Jeong\n",
    "- Ananya Krishnan\n",
    "- JohnWesley Pabalate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents and how they are measured\n",
    "- what you will be doing with the data\n",
    "- how performance/success will be measured\n",
    "---\n",
    "Due to factors such as rarity, demand, and general collector interest, Pokemon cards have significant fluctuations in value in the trading card market. The goal of this project is to develop a Markov Decision Process and Q learning model to predict and optimize the price of Pokemon cards with structured data sources from Kaggle. Our chosen datasets include important attributes such as card type, rarity, set generation, Pokemon abilities and strengths, and historical market prices. These features will help our model with analyzing trends and predicting the market value of these cards. \n",
    "\n",
    "Markov Decision Process provides a mathematical framework for modeling decision-making in dynamic environments, which are well suited for stochastic scenarios such as price fluctuations. In addition to MDPs, we will also leverage a reinforcement learning approach using the Q-learning algorithm to iteratively refine pricing strategies based on the datasets. The model will explore different pricing strategies to maximize long-term profitability.\n",
    "\n",
    "The model will be trained on historical card price trends (1999-2003) and tested against real data (2024) to test the performance of the model. Evaluation metrics will include profitability, policy effectiveness, and inventory effectiveness. The results of this project aim to provide users with a reliable tool for Pokemon card pricing and trend analysis backed by data. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Background\n",
    "\n",
    "Fill in the background and discuss the kind of prior work that has gone on in this research area here. **Use inline citation** to specify which references support which statements.  You can do that through HTML footnotes (demonstrated here). I used to reccommend Markdown footnotes (google is your friend) because they are simpler but recently I have had some problems with them working for me whereas HTML ones always work so far. So use the method that works for you, but do use inline citations.\n",
    "\n",
    "Here is an example of inline citation. After government genocide in the 20th century, real birds were replaced with surveillance drones designed to look just like birds<a name=\"lorenz\"></a>[<sup>[1]</sup>](#lorenznote). Use a minimum of 3 to 5 citations, but we prefer more <a name=\"admonish\"></a>[<sup>[2]</sup>](#admonishnote). You need enough citations to fully explain and back up important facts. \n",
    "\n",
    "Remeber you are trying to explain why someone would want to answer your question or why your hypothesis is in the form that you've stated. \n",
    "\n",
    "---\n",
    "The Pokémon trading card market has become increasingly valuable, with rare cards sometimes selling for thousands of dollars. Factors such as rarity, collector demand, and external market conditions influence these price fluctuations<a name=\"stiller\"></a>[<sup>[1]</sup>](#collectiblesnote). Some individuals have even turned Pokémon card trading into a full-time business. For example, former NFL player Blake Martinez retired from football to focus on reselling Pokémon cards, generating millions in revenue<a name=\"martinez\"></a>[<sup>[2]</sup>](#martinezsnote).\n",
    "\n",
    "One of the biggest challenges in this market is pricing strategy. Traditional methods, like fixed markup pricing, fail to capture the dynamic nature of supply and demand. Online marketplaces such as TCGPlayer and eBay operate as multisided platforms, where prices shift based on buyer interest, scarcity, and competitive listings. Research on platform pricing strategies shows that businesses optimize their pricing by adjusting costs based on user behavior, similar to how Pokémon card prices rise when collector demand increases<a name=\"platforms\"></a>[<sup>[3]</sup>](#platformsnote).\n",
    "\n",
    "Another key factor affecting Pokémon card prices is network effects. A card’s value can skyrocket if a popular YouTuber or competitive player features it in a video. This aligns with research on indirect network effects, where an increase in engagement from one group of users (buyers) raises value for another group (sellers and trading platforms)<a name=\"network\"></a>[<sup>[4]</sup>](#networknote). Traditional pricing models struggle to react quickly to these sudden shifts, leading to inconsistent valuations.\n",
    "\n",
    "To handle these challenges, researchers have applied AI-based dynamic pricing models in industries like e-commerce, stock trading, and airline ticketing. Studies have found that AI-driven models outperform static pricing strategies in these fields because they can analyze real-time demand and adjust prices accordingly<a name=\"ai-pricing\"></a>[<sup>[5]</sup>](#ai-pricingnote).\n",
    "\n",
    "A Markov Decision Process (MDP) is a useful tool for modeling price changes over time. It allows AI models to learn from past pricing decisions and optimize future choices, even in uncertain market conditions<a name=\"markov\"></a>[<sup>[6]</sup>](#markovnote).\n",
    "\n",
    "One effective reinforcement learning method is Q-learning, which helps adjust prices dynamically by learning from past trends and optimizing decisions for profitability<a name=\"qlearning\"></a>[<sup>[7]</sup>](#qlearningnote).Researchers have successfully used Q-learning in online retail and auction pricing, showing that it can set real-time prices more effectively than traditional methods<a name=\"ai-commerce\"></a>[<sup>[8]</sup>](#ai-commercenote).\n",
    "\n",
    "Very little research has used AI for pricing collectibles like Pokémon cards. Our goal is to apply reinforcement learning to make pricing more dynamic. By using past sales data and real-time trends, our model will set more accurate prices, helping sellers maximize profits and respond to market changes faster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once).\n",
    "\n",
    "---\n",
    "We want to build a dynamic pricing model that optimizes Pokemon trading card prices using reinforcement learning. Our goal is to maximize long term profits by balancing demand trends, market value and card rarity. Instead of fixed markups or rule based strategies, we want our model to use real-time market data in developing an optimal pricing strategy. \n",
    "The problem is: \n",
    "Quantifiable since the pricing decision can be expressed as a Markov Decision Process (MDP) with state variables (current price, demand, inventory), actions (price adjustments), and a reward function (profit over time). \n",
    "Measurable: We will use cumulative profit, price elasticity of demand, and inventory turnover rate as key metrics to evaluate the model’s effectiveness.\n",
    "Replicable: The model will be trained on historical card price trends (1999–2023) and tested against real 2024 data and simulated market environments to assess generalizability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "You should have a strong idea of what dataset(s) will be used to accomplish this project. \n",
    "\n",
    "If you know what (some) of the data you will use, please give the following information for each dataset:\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc will be needed\n",
    "\n",
    "If you don't yet know what your dataset(s) will be, you should describe what you desire in terms of the above bullets.\n",
    "\n",
    "---\n",
    "We are pulling most of our datasets from Kaggle, where we are specifically looking for datasets that include various specific attributes of a card, for example, type of card (whether it is holo or any unique designs), the rarity of the cards in the set, the generation of the card which may influence the rarity of it, the abilities and strengths of the Pokemon itself, and the prices that the cards can be resold for or the market prices that they are at.\n",
    "\n",
    "https://www.kaggle.com/datasets/adampq/pokemon-tcg-all-cards-1999-2023\n",
    "- This dataset provides data for all Pokemon trading cards from 1999 - 2023, allowing for analysis of cards spanning multiple series and generations. It also provides data on detailed attributes, abilities, attacks, rarity, legalities, and other relevant information, going into detail the characteristics of each card.\n",
    "- This dataset consists of 29 columns and was last updated a year ago, consisting of pretty recent data on Pokemons.\n",
    "  \n",
    "https://www.kaggle.com/datasets/shivd24coder/pokemon-card-collection-dataset\n",
    "- This dataset provides similar data to the one above, however, there is additional information in this dataset that would provide useful information in analyzing how pricing of cards can be determined, where attributes like the artist of the card, the image of the card, and pricings that can be pulled from URLs given that directly link to the official card on TCGPlayer.com.\n",
    "- This dataset consists of 5 columns, two of which are URLs that link to the TCGPlayer website, which was last updated a year ago, showing more recent data on Pokemons as well.\n",
    "\n",
    "https://www.kaggle.com/datasets/jacklacey/pokemon-trading-cards\n",
    "- This dataset was specifically chosen based on the attribute that indicates specific prices of each card, however, the prices listed in this dataset are limited in which the prices are fixed values and don’t show market fluctuations, which doesn’t allow flexibility in using this dataset.\n",
    "- This dataset consists of 5 columns, which this dataset was last updated 3 years ago, where some of the data may be slightly outdated.\n",
    "\n",
    "https://github.com/wjsutton/pokemon_tcg_stockmarket\n",
    "- This dataset allows for a deeper look into the Pokemon Trading Card stock market, allowing us to see the daily pricing of Pokemon cards, which allows for us to be able to evaluate trends in fluctuations of prices for the cards. This dataset is directly sourced from the official TCGPlayer website with variables that focus on the card identification, rarity, and market price. This dataset was last updated 4 years ago, which may bring it more outdated information, which we can explore and see how we can use it as a resource for finding more recent data on the market of the Pokemon cards.\n",
    "\n",
    "Much of this data will need to be cleaned and formatted with the same guidelines so that they can be easily used. Some datasets may even be merged or combined to allow for more concise usage of data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Why might your solution work? Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. \n",
    "\n",
    "---\n",
    "We will use reinforcement learning to create an agent that adjusts card prices dynamically to maximize long-term revenue. The model will be trained using Q-learning or Deep Q Networks (DQN). Instead of just forecasting prices, we want the model to actively make pricing decisions based on past and real-time data. We will train the model on long-term historical data to capture price fluctuations, and test it on recent months (e.g., 2024 Q1) to evaluate real-world effectiveness. We will compare it with a basic rule-based fixed pricing model (e.g., cost based pricing that involves adding a markup to the cost of production to determine a selling price). This approach is effective because reinforcement learning optimizes decision-making in dynamic environments, allowing the model to adapt to shifting market conditions and maximize profitability over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms).\n",
    "\n",
    "---\n",
    "We will evaluate the profitability by comparing cumulative profit of the reinforcement learning-based strategy vs. the benchmark model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination. Get creative!\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "---\n",
    "Although predicting the price of Pokemon cards does not involve handling sensitive personal information, there are ethical considerations that we must take into account, including data accuracy, bias, and potential market manipulation. \n",
    "\n",
    "The datasets that we chose to use in this project are sourced from different Kaggle repositories, some of which contain the historical records of Pokemon cards, resale prices, and market trends over time. Some datasets provide fixed values, while the others include market fluctuations. Relying too heavily on fixed-prices can prevent the model’s ability to capture real-time prices, resulting in less accurate predictions.\n",
    "\n",
    "Another important  consideration is data bias. The datasets mainly draw information from  platforms like TCGPlayer.com, which may fail to represent different market segments including private sales, regional trading events, and local card shops. This could result in the model to favor one type of card sales over another, leading to skewed predictions. To mitigate this, we will focus on analyzing data distributions. \n",
    "\n",
    "Market manipulation is also a risk that must be considered. The model’s predictions could be used unethically to inflate or  deflate card prices. In order to prevent such issues from occurring, we need to ensure transparency in how the model works and makes predictions. Users should be informed that the model uses past data and trends, ensuring that the model should be used as a decision-support tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
    "* *Team Expectation 1*\n",
    "* *Team Expectation 2*\n",
    "* *Team Expecation 3*\n",
    "* ...\n",
    "\n",
    "---\n",
    "Communication\n",
    "- We will use iMessage as the primary communication platform, with a dedicated group chat for updates, brainstorming, and general discussion.\n",
    "- Frequent meetings will be held and we will alternate between virtual (Zoom) and in-person when agreed upon. These meetings will discuss current progress, address any problems, and re-align on upcoming tasks. If a member cannot attend, they should review the agenda beforehand and provide updates asynchronously. Every meeting will have an agenda shared beforehand.\n",
    "  \n",
    "Decision-Making\n",
    "- We’ll use a consensus approach when possible; otherwise, decisions will be made by a majority vote.\n",
    "- Specific roles will have specific duties. (ie. “Data Wrangler” for data decisions. This allows efficient action while maintaining accountability.\n",
    "  \n",
    "Task Assignment\n",
    "- Define clear roles based on strengths and preferences. Roles should be flexible, but each member will have a specific task that they should be working on.\n",
    "Tasks will be assigned at each meeting, organized by category (ie. research, code, editing).\n",
    "- Each member is responsible for updating the group when they start and complete a task.\n",
    "- For each code or document submission, we will review it to ensure accuracy.\n",
    "\n",
    "Timeline and deadlines\n",
    "- Set project milestones and deadlines for each major task, with deadlines adjusted as needed to keep on track.\n",
    "- Bi-weekly progress checks will ensure everyone meets individual and group timelines. Adjustments can be made depending on the situation.\n",
    "- Each member is expected to update the group if they are having trouble meeting the deadline or are unable to make scheduled meetings, ideally three days in advance to allow for rescheduling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with something meaningful that is appropriate for your needs. It doesn't have to be something that fits this format.  It doesn't have to be set in stone... \"no battle plan survives contact with the enemy\". But you need a battle plan nonetheless, and you need to keep it updated so you understand what you are trying to accomplish, who's responsible for what, and what the expected due dates are for each item.\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/14  |  1 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; Complete Project Proposal| \n",
    "| 2/21  |  1 PM | Do background research on topic and also research into suitable models | Look into datasets and how best to use them. Start discussing formatting and cleaning data| \n",
    "| 2/25  | 5 PM  | Starting understanding the datasets and begin process of cleaning and formatting | DDiscuss Wrangling and possible analytical approaches; Assign group members to lead each specific part especially for EDA and each dataset |\n",
    "| 3/01 | 2 PM  | Wrangle Data, do some EDA | Review/Edit wrangling/EDA; Discuss Analysis Plan |\n",
    "| 3/07 | 12 PM  | Finalize wrangling/EDA; Begin programming for project (Cruyff) | Discuss/edit project code; Complete project and write up |\n",
    "| 3/14  | 1 PM  | Complete analysis; Draft results/conclusion/discussion (Carlos)| Discuss/edit full project |\n",
    "| 3/19  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "\n",
    "<a name=\"shillernote\"></a>[<sup>[1]</sup>](#shiller) Shiller, Robert J. *Narrative Economics: How Stories Go Viral and Drive Economic Events.* Princeton University Press, 2019. [https://press.princeton.edu/books/hardcover/9780691182292/narrative-economics](https://press.princeton.edu/books/hardcover/9780691182292/narrative-economics).\n",
    "\n",
    "<a name=\"martineznote\"></a>[<sup>[2]</sup>](#martinez) Martinez, Blake. *\"How I Made Millions Selling Pokémon Cards After Leaving the NFL.\"* CNBC, 26 Apr. 2023. [https://www.cnbc.com/2023/04/26/blake-martinez-pokemon-card-side-hustle-company-brings-in-millions.html](https://www.cnbc.com/2023/04/26/blake-martinez-pokemon-card-side-hustle-company-brings-in-millions.html). Accessed 14 Feb. 2025.\n",
    "\n",
    "<a name=\"platformsnote\"></a>[<sup>[3]</sup>](#platforms) Rochet, Jean-Charles, and Jean Tirole. *\"Platform Competition in Two-Sided Markets.\"* *Journal of the European Economic Association*, vol. 1, no. 4, 2003, pp. 990-1029. [https://academic.oup.com/jeea/article/1/4/990/2280902](https://academic.oup.com/jeea/article/1/4/990/2280902).\n",
    "\n",
    "<a name=\"networknote\"></a>[<sup>[4]</sup>](#network) Evans, David S., and Richard Schmalensee. *\"The Economics of Two-Sided Markets.\"* *Review of Network Economics*, vol. 6, no. 2, 2007, pp. 1-26.\n",
    "\n",
    "<a name=\"ai-pricingnote\"></a>[<sup>[5]</sup>](#ai-pricing) Bertsimas, Dimitris, and Nathan Kallus. *\"From Predictive to Prescriptive Analytics.\"* *Management Science*, vol. 65, no. 3, 2019, pp. 1027-1049. [https://pubsonline.informs.org/doi/10.1287/mnsc.2018.3253](https://pubsonline.informs.org/doi/10.1287/mnsc.2018.3253).\n",
    "\n",
    "<a name=\"markovnote\"></a>[<sup>[6]</sup>](#markov) Puterman, Martin L. *Markov Decision Processes: Discrete Stochastic Dynamic Programming.* John Wiley & Sons, 1994. [https://onlinelibrary.wiley.com/doi/chapter-epub/10.1002/9780470316887.fmatter](https://onlinelibrary.wiley.com/doi/chapter-epub/10.1002/9780470316887.fmatter).\n",
    "\n",
    "<a name=\"qlearningnote\"></a>[<sup>[7]</sup>](#qlearning) Watkins, Christopher J. C. H., and Peter Dayan. *\"Q-learning: Model-Free Reinforcement Learning.\"* *Machine Learning Journal*, vol. 8, no. 3-4, 1992, pp. 279-292. [https://link.springer.com/article/10.1007/BF00992698](https://link.springer.com/article/10.1007/BF00992698).\n",
    "\n",
    "<a name=\"ai-commercenote\"></a>[<sup>[8]</sup>](#ai-commerce) Tesauro, Gerald, and Jeffrey O. Kephart. *\"Pricing Strategies Using Q-Learning in E-Commerce.\"* *AAAI Conference on Artificial Intelligence*, 2002. [https://www.researchgate.net/publication/2820310_Pricing_in_Agent_Economies_Using_Multi-Agent_Q-Learning](https://www.researchgate.net/publication/2820310_Pricing_in_Agent_Economies_Using_Multi-Agent_Q-Learning).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
